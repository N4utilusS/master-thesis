\documentclass[oneside, a4paper, 12pt]{memoir}

\usepackage[utf8]{inputenc}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[marginparwidth=28mm]{geometry}
\usepackage[pdftex]{graphicx}
\usepackage{tikz}

\usepackage{filecontents}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}
\usepackage{newpxtext,newpxmath}
\usepackage[babel=true]{csquotes}
\usepackage[round]{natbib}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{comment}
\usetikzlibrary{arrows,automata}
\usepackage{ccaption}
\usepackage{url}
\usepackage{flafter}
\usepackage{pgfplots}
\usepackage[figure]{algorithm2e}

% Logos
\newcommand{\ulb}{\includegraphics[scale=1.1]{PageDeGarde_MFE/logo_ULB2.pdf}}
\newcommand{\polytech}{\includegraphics[scale=0.35]{PageDeGarde_MFE/logo_polytech_FR.pdf}}

% Polices
\definecolor{ULBblue}{rgb}{0,0.2196,0.5765}
\newcommand{\fontTitle}{\sffamily \Huge\selectfont \color{ULBblue}}
\newcommand{\fontSubtitle}{\sffamily \LARGE \selectfont \color{ULBblue}}
\newcommand{\fontText}{\sffamily \selectfont}
\newcommand{\fontColor}{\sffamily \selectfont \color{ULBblue}}

% Titre
\newcommand{\titleA}{\fontTitle{Human - Robots Swarms Interaction}} % Titre identique au titre remis au secrétariat
%\newcommand{\titleB}{\fontTitle{Deuxième ligne de titre du mémoire}} % (dans la langue de rédaction a priori)
% Sous-titre
\newcommand{\subtitleA}{\fontSubtitle{An Escorting Robot Swarm that Diverts a Human}}
\newcommand{\subtitleB}{\fontSubtitle{away from Dangers one cannot perceive.}}
% Titre du diplôme
\newcommand{\diplomaA}{\fontText{Mémoire présenté en vue de l’obtention du diplôme}} % A laisser en Français
\newcommand{\diplomaB}{\fontText{d'Ingénieur Civil en Informatique à finalité Intelligence Computationnelle}}

% Etudiant
\newcommand{\student}{\textbf{\sffamily \large Anthony Debruyn}}

% Supervision
\newcommand{\promAa}{\fontColor{Directeur}}
\newcommand{\promAb}{\fontText{Professeur Mauro Birattari}}
\newcommand{\promBa}{\fontColor{Co-Promoteur}}
\newcommand{\promBb}{\fontText{Professeur Marco Dorigo}}
\newcommand{\promCa}{\fontColor{Superviseur}}
\newcommand{\promCb}{\fontText{Gaëtan Podevijn, Andreagiovanni Reina}}
\newcommand{\deptA}{\fontColor{Service}}
\newcommand{\deptB}{\fontText{IRIDIA}}

% Année académique
\newcommand{\yearA}{\fontColor{Année académique}}
\newcommand{\yearB}{\fontText{2014 - 2015}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MY NEW COMMANDS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\quoto}[2]{
\begin{quotation}
\textit{\enquote{#1} - #2}
\end{quotation}
}

\newcommand{\quot}[1]{\textit{\enquote{#1}}}


\newcommand{\epuck}[3][0] % [angle]{x}{y} avec angle optionel
{
	\draw [very thick, fill=white] (#2,#3) circle [radius=0.5];
	\draw [very thick, rotate around={#1:(#2,#3)}] (#2-0.25,#3-0.433) -- (#2,#3+0.45) -- (#2+0.25,#3-0.433);
}

\newcommand{\epuckred}[3][0] % [angle]{x}{y} avec angle optionel
{
	\draw [very thick, fill=orange] (#2,#3) circle [radius=0.5];
	\draw [very thick, rotate around={#1:(#2,#3)}] (#2-0.25,#3-0.433) -- (#2,#3+0.45) -- (#2+0.25,#3-0.433);
}

\newcommand{\epuckblue}[3][0] % [angle]{x}{y} avec angle optionel
{
	\draw [very thick, fill=RoyalBlue] (#2,#3) circle [radius=0.5];
	\draw [very thick, rotate around={#1:(#2,#3)}] (#2-0.25,#3-0.433) -- (#2,#3+0.45) -- (#2+0.25,#3-0.433);
}

\newcommand{\human}[3][0] % [angle]{x}{y}
{
	\draw [very thick, fill=white, rotate around={#1:(#2,#3)}] (#2-1,#3+0.5) ellipse (0.25cm and 0.5cm);
	\draw [very thick, fill=white, rotate around={#1:(#2,#3)}] (#2+1,#3+0.5) ellipse (0.25cm and 0.5cm);
	\draw [very thick, fill=white, rotate around={#1:(#2,#3)}] (#2,#3) ellipse (1.5cm and 0.75cm);
	\draw [thick, fill=white, rotate around={#1:(#2,#3)}] (#2-0.05,#3+1) -- (#2,#3+1.1) -- (#2+0.05,#3+1);
	\draw [very thick, fill=white, rotate around={#1:(#2,#3)}] (#2,#3+0.5) circle [radius=0.5cm];
}


\let\oldCaption\caption
\renewcommand{\caption}[2]{
\oldCaption[#1]{{\small\sffamily\bfseries #1:} #2}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEW STYLES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\captiondelim{ -- }
\captionnamefont{\small\sffamily\bfseries}
\captiontitlefont{\small\sffamily}
\precaption{\rule{\linewidth}{0.4pt}\\}
%\setcounter{secnumdepth}{3}
%\setcounter{tocdepth}{3}
\maxsecnumdepth{subsubsection}% 'secnumdepth' is reset by \mainmatter. You should use 'maxsecnumdepth'.
\maxtocdepth{subsubsection}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THE DOCUMENT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\frontmatter

	\thispagestyle{empty}
	\newgeometry{top=2.5cm, bottom=1.5cm, left=2.5cm, right=1cm}
	\setlength{\unitlength}{1mm}
	\noindent\begin{picture}(175,257)
	
		\put(0,245){\polytech}
		\put(153,139.5){\ulb}
		
		\put(8,155){\makebox(150,10)[l]{\titleA}}
%		\put(8,145){\makebox(150,10)[l]{\titleB}}
		\put(8,135){\makebox(150,10)[l]{\subtitleA}}
		\put(8,125){\makebox(150,10)[l]{\subtitleB}}
		
		\put(0,75){
		\begin{tikzpicture}[scale=0.1]
		\fill [fill=ULBblue](0,0) rectangle (0.8,90);
		\fill [fill=ULBblue](0,47) rectangle (152,47.8);
		\end{tikzpicture}}
		
		\put(8,110){\makebox(150,5)[l]{\diplomaA}}
		\put(8,105){\makebox(150,5)[l]{\diplomaB}}
		
		\put(8,75){\makebox(150,10)[l]{\selectfont \student}}
		
		\put(8,44){\makebox(80,5)[l]{\promAa}}
		\put(8,39){\makebox(80,5)[l]{\promAb}}
		\put(8,31){\makebox(80,5)[l]{\promBa}} % Commenter la ligne si pas nécessaire
		\put(8,26){\makebox(80,5)[l]{\promBb}} % Commenter la ligne si pas nécessaire
		\put(8,18){\makebox(80,5)[l]{\promCa}} % Commenter la ligne si pas nécessaire
		\put(8,13){\makebox(80,5)[l]{\promCb}} % Commenter la ligne si pas nécessaire
		\put(8,5){\makebox(80,5)[l]{\deptA}}
		\put(8,0){\makebox(80,5)[l]{\deptB}}
		
		\put(145,5){\makebox(30,5)[r]{\yearA}}
		\put(145,0){\makebox(30,5)[r]{\yearB}}
	
	\end{picture}
	\restoregeometry
	
% Template conçu par Benjamin Vanhemelryck et revu par François Bronchart - Mai 2013
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEFORE TEXT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Acknowledgements}

\begin{figure}
	\centering\begin{tikzpicture}
	%Mauro, Gaëtan, Giovanni, Anthony, Lorenzo, Brian, Family and friends.
	\epuck{-5}{12}
	\epuck{-5}{10}
	\epuck{-5}{8}
	\epuck{-5}{6}
	\epuck{-5}{4}
	\epuck{-5}{2}
	\epuck{-5}{0}
	\epuck[60]{-2.5}{-4.33}
	\epuck[120]{2.5}{-4.33}
	\epuck[180]{5}{0}
	
	\draw (-4,12) node[anchor=west] {Mauro Birattari};
	\draw (-4,10) node[anchor=west] {Gaëtan Podevijn};
	\draw (-4,8) node[anchor=west] {Andreagiovanni Reina};
	\draw (-4,6) node[anchor=west] {Anthony Antoun};
	\draw (-4,4) node[anchor=west] {Brian Delhaisse};
	\draw (-4,2) node[anchor=west] {Lorenzo Garattoni};
	\draw (-4,0) node[anchor=west] {Family \& Friends};
	
	\end{tikzpicture}
\end{figure}

\chapter{Résumé}
\chapter{Summary}
\tableofcontents
\listoffigures
\listoftables

\listoftodos

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TEXT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mainmatter
\chapter{Introduction}

[I'll do this and this... blah blah blah...]

\begin{comment}
INTRO
Il faut respecter un fil conducteur dans la thèse. D'abord teaser le contenu dans l'introduction : "j'ai fait un super controller qui peut permettre à des robots de protéger un humain. Ca n'avait jamais été fait avant. J'utilise une swarm. J'ai de très bons résultats. On a testé avec des vrais robots, et l'humain est empêché d'accéder à des zones dangereuses.

STATE OF THE ART
Voici ce qui existe dans le domaine human - robot interaction. Rien de tout cela ne me satisfait et répond à mon problème.

Idem pour human - swarm interaction.

Je vais donc essayer de répondre au problème avec une nouvelle solution... Elle est meilleure que les autres pour ce problème car...

SOLUTION
Pour parvenir à mes fins, j'ai dû implémenter un controller. J'ai choisi virtual physics car ces avantages et inconvénients. C'est ce gars qui l'a inventé, et voici les travaux utilisant cette technique.

\end{comment}

As swarm robotic systems are mostly destined to operate on risky floors, unknown environment, it would seem logical to consider their application in exploration and/or protection missions. However, at the time of writing this thesis, we could not find any study on the subject. Exploration experiments never included a human, or other living organism. The object of this thesis is to address this lack of study by designing and implementing a protective behaviour executed by a robotic swarm.
	
	The human operator is here part of the swarm system. The swarm has to protect him by preventing him from going into dangerous areas, in the same way a group of bodyguards protects someone. The swarm has to follow the operator anywhere to ensure permanent protection.\\
	
	We believe this work to be important since it could lay the foundations of a new branch in swarm engineering: human protection, escort or swarm turn-by-turn navigation.
	
	An article will be written to expose this research to the rest of the swarm robotics community.

\chapter{State of the Art}
	
	In this section, we will discuss the problem that led to the creation of this thesis by first providing the reader with some general insight in the world of swarm robotics and swarm intelligence. Then we will focus on specific parts of these domains of study: feedbacks between human and single robot, and human and robots' swarm.
	
	\section{Human - Robot Interaction}
	
	[Work related to what I do (detect humans, protection, follow person). Conclude on why it cannot be applied to my problem.]

	\section{Swarm Robotics}
	
	[Flocking with a guide, and then Pattern Formation with a Guide. Work related to what I do. Conclude on why it cannot be applied to my problem. Make connections with this part later in the document. S'inspirer de Brambilla pour les flock et pattern.]
	
	This section and the next one are largely inspired by~\citet{brambilla2013swarm}, a reviewing article on swarm engineering. For \citet{csahin2005swarm}, swarm robotics is defined as \quot{the study of how large numbers of relatively simple physically embodied agents can be designed such that a desired collective behavior emerges from the local interactions among agents and between the agents and the environment}~\citep{csahin2005swarm}. Swarm robotics can be separated from other robotic studies by the following characteristics \citep{brambilla2013swarm}:
	
\begin{itemize}
\item Robots are \emph{autonomous}
\item Robots evolve \emph{in the environment} and can interact with it
\item Robots' interactions are \emph{local} (sensors and communications)
\item No \emph{centralised control} or \emph{global knowledge}
\item Robots \emph{cooperate} to achieve a certain goal
\end{itemize}

As in this field of study, one is always looking for \emph{robust}, \emph{scalable} and \emph{flexible} systems, the main source of inspiration is the group of social animals: ants, birds, fishes, ... When some of these simple animals gather in groups, they are able to perform tasks that could not be achieved individually (collective behaviour emerges from local interactions). Below are listed the definitions of these three terms \citep{brambilla2013swarm}:

\label{def:robustness_scalability_flexibility}
\begin{description}
\item[Robustness:] Resistance against \emph{loss of group entities}. One can increase it by adding redundancy or remove the need for a leader.
\item[Scalability:] Low variation in the performance of a system with respect to the \emph{size of the system}. It can be increased by encouraging local interactions, such as sensing and communications.
\item[Flexibility:] Low variation in the performance of a system with respect to the \emph{type of environment or the task}.
\end{description}

With these definitions in mind, we can explain swarm engineering as:

\quoto{Swarm engineering is an emerging discipline that aims at defining systematic and well founded procedures for modeling, designing, realizing, verifying, validating, operating, and maintaining a swarm robotics system.}{\cite{brambilla2013swarm}}

\citet{kazadi2000swarm} points out that \quot{to the swarm engineer, the important points in the design of a swarm are that the swarm will do precisely what it is designed to do, and that it will do so reliably and on time} \citep{kazadi2000swarm}.
	
	\subsection{Human - Robots Swarm Interaction}
	
	[Work related to what I do (detect humans, protection, follow person). Conclude on why it cannot be applied to my problem.]
	
	Human - Robotic swarm interaction is the study of how humans can interact with a swarm to control it and receive feedback from it \citep{brambilla2013swarm}. A proper feedback is needed by the operator in order to make the right decisions. Since swarms must ideally be autonomous and make decisions in a distributed way, it is difficult to insert a communication with a human operator in the system to gain control.\\
	
	Currently, little attention has been devoted to the study of the interaction between humans and robotic swarms, how one can send instructions and receive feedback. People investigating in the field encounter many difficulties, such as the difference of perspective between the swarm and the human operator (the human only observes the global collective behaviour, not the local interactions or individual behaviours driving the robots), the simplicity of the hardware found on the robots, or the efficient synthesis of all the information sent by the robots. All the existing types of interactions in the literature present a major disadvantage: they require an extra layer between the group of robots and the human. This requirement might not always be satisfied when we remember that swarms like this are mostly destined to evolve in an unknown environment. The monitoring equipment necessary to operate the swarm may not be safely deployed. Furthermore, a synthesis of all the local information pieces must be done in order to provide an understandable state of the system to the human. A supplementary step that involves modelling, additional overheads and perhaps heavy computations, and the gathering of all information at a central point (eliminating by the way the distributed and not centralised properties of the swarm system) \citep{podevijn2012self}.\\
	
	\citet{daily2003world} used a head-mounted display and augmented reality to add information right on top of the robot in the environment itself, suppressing the need for an additional display. \citet{baizid2009human} proposed a platform to interact with multiple robots simultaneously through a graphical user interface, or a head-mounted display, virtual reality etc. They also studied how virtual reality abstraction affected the human perception and cognitive capabilities, i.e, they created a virtual environment by filtering useless information. \citet{mclurkin2006speaking} developed an centralised graphical user interface taking inspiration from real-time strategy video games, where one must control armies. They also imagined a feedback approach based on LEDs and sounds. The robots transmit their internal state by applying to their LEDs and sound system a defined pattern, recognisable by the operator, now able to quickly understand the state of the swarm without looking at a supplementary interface.
	
	\citet{podevijn2012self} argue that self-organised mechanism, as those ruling the behaviour of the swarm, should be used to provide feedback to the operator. They suggest that the best entity which could communicate the status of the system and the whole swarm is the swarm itself. They performed experiments using colour feedback to distinguish different internal states and split the swarms into groups to tackle different tasks.\\
	
\begin{comment}
[Currently, only little research on feedback between human and robots swarms. That research is focused on interaction with an additional layer... Mostly unidirectional communication (human to robots). Need new types of interactions for new applications. Robots to human (guide). Write about article saying self-organised feedback is better.] \todo{Remove}
\end{comment}

	
		
		
	
	As swarm robotic systems are mostly destined to operate on risky floors, unknown environment, it would seem logical to consider their application in exploration and/or protection missions. However, at the time of writing this thesis, we could not find any study on the subject. Exploration experiments never included a human, or other living organism. The object of this thesis is to address this lack of study by designing and implementing a protective behaviour executed by a robotic swarm.
	
	The human operator is here part of the swarm system. The swarm has to protect him by preventing him from going into dangerous areas, in the same way a group of bodyguards protects someone. The swarm has to follow the operator anywhere to ensure permanent protection.\\
	
	We believe this work to be important since it could lay the foundations of a new branch in swarm engineering: human protection, escort or swarm turn-by-turn navigation.

%////////////////////////////////////////////////////////////
\chapter{An Escorting Swarm}
%////////////////////////////////////////////////////////////
\label{chap:escorting_swarm}

\begin{comment}
[\textcolor{red}{This is the core of the thesis. It must be a standalone. The reader must understand my work through the reading of this part only!} Explain the solution independently of the tools used to implement the solution. Make the reader understand that this problem is new.

On veut protéger un humain qui ne voit rien comme danger. Solution: un swarm de robots perçu par l'humain avec UN WEARABLE DEVICE (en parler après avoir mentionné le challenge correspondant).

Challenges: une fois qu'on connait la solution générale, parler des challenges, genre comment reconnaître un humain?]\\
\end{comment}

In this chapter, we will explore the investigated problem and the proposed solution at a high level of description. In the next chapter, we will describe the implementation details.

	\section{The Problem}
	
	\begin{comment}
	[Human walking in unknown environment. Team to assist, perceive danger that the human cannot. Graphical example, human goes from A to B with dangers (keep it simple as demo). Write text about an image showing the problem. Mined field, real applications.]\\
	\end{comment}
	
	Since the early days, human beings have explored new territories to expand their control and get a better understanding of the world surrounding them. Among those new landscapes, some were relatively safe but some were dangerous. To overcome this, we have invented equipment, suits, and other kinds of protections. We tried, with this work, to contribute to the study of these solutions.\\
	
	Figure \ref{fig:unknown_environment} shows a graphical representation of a possible scenario. The dangerous areas are the red circles. Those areas could be radioactive areas, mine fields, or any other invisible threats. The human must travel from point \emph{A} to \emph{B} without being hurt by the danger contained in these areas. The human cannot perceive them. The protection created should prevent the user from going inside those areas.\\
	
	Exploration is not the only real application for the proposed solution that comes into mind. Rescue in disaster areas would also benefit from it (evacuation of people to safe zones, etc). The solution should be able to constantly protect the person using it, and constantly provide feedback. It should be robust and fit to the destination environment.
	
%	\begin{figure}\centering
%		\begin{tikzpicture}
%			\draw [dashed] (0,0) circle [radius=1];
%			\draw (0,0) node[scale=2]{A};
%			
%			\draw [dashed] (10,10) circle [radius=1];
%			\draw (10,10) node[scale=2]{B};
%			
%			\draw [dashed, fill=red] (5,5) circle [radius=3];
%			\draw (5,5) node[scale=5]{!};
%			
%			\draw [->, very thick, green] (0,1.5) to [out=90,in=180] (8.5,10);
%			\draw [->, very thick, green] (1.5,0) to [out=0,in=270] (10,8.5);
%			\draw [->, very thick, red] (1.5, 1.5) -- (2.5,2.5);
%		\end{tikzpicture}
%	\end{figure}
	
	\begin{figure}\centering
		\begin{tikzpicture}
			\draw [dashed] (0,0) circle [radius=0.5];
			\draw (0,0) node[scale=2]{A};
			
			\draw [dashed] (12,8) circle [radius=0.5];
			\draw (12,8) node[scale=2]{B};
			
			\draw [dashed, fill=red] (3,0) circle [radius=1.2];
			\draw (3,0) node[scale=5]{!};
			
			\draw [dashed, fill=red] (4.5,3) circle [radius=1.5];
			\draw (4.5,3) node[scale=5]{!};
			
			\draw [dashed, fill=red] (6,7) circle [radius=1];
			\draw (6,7) node[scale=5]{!};
			
			\draw [dashed, fill=red] (10,6) circle [radius=1];
			\draw (10,6) node[scale=5]{!};
			
			\draw [->,very thick, orange] (0.75,0) to [out=0,in=180] (1.5,0) to [out=90,in=200] (3,2) to [out=120,in=-150] (5,6) to [out=-25, in=-160] (8.75,5.5) to [out=110, in=180] (11.25,8);
			
			\draw [very thick] (1,0.433) arc [radius=0.5, start angle=120, end angle=180];
			\draw [very thick] (1,0.433) arc [radius=0.5, start angle=60, end angle=0];
			\draw [very thick] (1,0.433) -- (1,0.933);
			\draw [very thick, fill=white] (1,0.433+0.625) circle [radius=0.125];
			\draw [very thick] (1,0.433+0.375) arc [radius=0.5, start angle=60, end angle=0];
			\draw [very thick] (1,0.433+0.375) arc [radius=0.5, start angle=120, end angle=180];
		\end{tikzpicture}
		
		\caption{Unknown Dangerous Environment}{This image illustrates an environment, observed from above, in which a human must move from point \emph{A} to point \emph{B} while avoiding invisible dangerous areas. \emph{A} is the start location, \emph{B} is the goal ant the red circles represent dangerous zones. We provide in this thesis a solution to guarantee safeness in such circumstances. Possible applications for this type of solutions are: mine fields crossing and cleaning, radioactive areas avoidance,...}
		\label{fig:unknown_environment}
	\end{figure}
	
	
	\section{Solution}
	
	\begin{comment}
	[Solution at high level and the problems I will have to solve? Faire le lien avec flocking et pattern. \textcolor{red}{The swarm is extending the perception capabilities of the human.} Challenges.
	On a choisit d'utiliser des swarms car... Equidistance par rapport à l'humain. Carac du système. Pb pour détecter l'humain...avec solution. Agent de la swarm doit être capable de détecter le danger. Humain perçoit la swarm qui elle perçoit le danger.]
	\end{comment}
	
	The solution we propose involves the use of a swarm of robots. Swarm robotics seems fit to this kind of application, since it is compatible with unknown environments thanks to its flexible, robust and scalable characteristics \citep{brambilla2013swarm}. In case of failure of one or a few robots, the system would continue to provide sufficient performance thanks to its scalability and robustness.\\
	
	The swarm of robots forms a round shield around a user. The round shield formed by the swarm enables a 360° protection of the user. All the robots try to stay at the same distance from each other and the human (except when there is a danger). To achieve this, the final solution relies on the pattern formation theory widely used in swarm robotics\todo{Parler du flocking et pattern.}. The corresponding techniques will be explained in the next chapter with more details. If the number of robots is not high enough to form a complete circle, an arc is formed at the front to always shield the most critical zone.\\
	
	As shown on figure \ref{fig:swarm_preventing}, the robots in contact with a dangerous zone will report the danger through visual communication with the human. Here the robots light on their orange LEDs and stay on the boundary of the zone to prevent the human from getting into it. Since the human cannot see the danger, and only the robots can, we can see that the swarm is increasing the perception capabilities of the human.\\
		
	\begin{figure}\centering
		\begin{tikzpicture}
		
		%	\foreach \i in {\xMin,...,\xMax} {
		%        \draw [very thin,gray] (\i,\yMin) -- (\i,\yMax)  node [below] at (\i,\yMin) {$\i$};
		%    }
		%    \foreach \i in {\yMin,...,\yMax} {
		%        \draw [very thin,gray] (\xMin,\i) -- (\xMax,\i) node [left] at (\xMin,\i) {$\i$};
		%    }
			
			\draw [very thick, dashed, fill=red] (2,6) arc [radius=6, start angle=180, end angle=270];
			
			\human{1}{0}
			
			\draw [dashed] (1,0) circle [radius=4];
			
			\epuckred{2.5}{3.5}
			\epuckred{3.5}{2}
			\epuckred{4.9}{0.8}
			\epuck{4.9}{-1}
			\epuck{4}{-2.5}
			\epuck{2.5}{-3.6}
			\epuck{-2.9}{-1}
			\epuck{-2}{-2.5}
			\epuck{-0.5}{-3.6}
			\epuck{-2.9}{1}
			\epuck{-2}{2.5}
			\epuck{-0.5}{3.6}
					
		\end{tikzpicture}
		
		\caption{Swarm Prevention}{This figure is a symbolic representation of a human helped by a swarm. The circles with a triangle inside are representations of a robot. The swarm tells the human that a dangerous zone is located at the front right by visual communication (here the robots change their colour to red). The swarm stays at the boundary to form a \enquote{shield}. The direction taken by each individual in the swarm is given by the triangle inside (here heading north).}
		\label{fig:swarm_preventing}
	\end{figure}
	
	One issue that had to be resolved was the detection of the human by the robots. As \citet{podevijn2012self} suggested, the interface between the human and the robots swarm should be restricted to the strict minimum because in the field the infrastructure needed to operate the swarm might not be easy to build and manipulate. The swarm should handle the communication on its own. Furthermore, any centralised control system would break the distributed and robust feature of swarm robotics. As a big infrastructure such as a tracking system, or any interface of the same kind would have been difficult to use in real life applications, we designed and implemented a compact, wearable device that allows a human to be recognised by the robots: a pair of shoes.\\
	
	Figure \ref{fig:shoes} illustrates the use of the shoes (no user is wearing them to not occlude the field of view). In figure \ref{fig:shoes} (left), the robots have just recognised the shoes thanks to the LED system inside and begin to move in order to form a circle around the shoe. n figure \ref{fig:shoes} (right), we show an example of one configuration obtained after 3 minutes, viewed from above.\\
	
	\begin{figure}
		\begin{minipage}[c]{0.6\textwidth}
			\includegraphics[trim=200px 300px 140px 300px, clip=true, height=5cm]{images/shoes.jpg}
		\end{minipage}
		\hfill
		\begin{minipage}[c]{0.39\textwidth}
			\hfill\includegraphics[trim=380px 230px 380px 360px, clip=true, height=5cm]{../Experiments/No_Human/6end.png}
		\end{minipage}				
		
		\caption{The Shoes}{This picture shows a prototype of the shoes viewed from above, and the robots interacting with it. The interaction is realised through the recognition of the colours, one for each shoe, indicating left or right side. This pair of shoes enables the robots to locate the user, allowing them to evolve at the target distance from him/her. On the left image, the robots are still in the process of placing themselves in a correct circle. The right image depicts the situation after a 3 minutes experiment where the robots were initially placed in lines around the shoes. Objects are put on the shoes to close the lights switch (normally activated by the weight of the user).}
		\label{fig:shoes}
	\end{figure}

Our objective in this thesis, is to present an innovative protection using swarm robotics. The results obtained with real robots are presented in the chapter \ref{chap:experiments}.

%////////////////////////////////////////////////////////////
\chapter{Implementation}
%////////////////////////////////////////////////////////////

	This chapter details the solution to the given problem and all the choices that resulted in it. The explanation will take a top-down approach, first reviewing the hardware and the general code architecture. It will then go deeper in the details.\\
	
	The first question one could ask is: why swarm robotics for such an application? The answer to that question lays in section \ref{def:robustness_scalability_flexibility}. Robustness, scalability and flexibility are characteristics that make swarms of robots really interesting in unknown environments \citep{brambilla2013swarm}.\todo{Insert more justification?} In case one of the agents is broken, we do not want to see the whole system collapse and leave the human unattended. Flexibility guarantees that the solution will work in different conditions, environments, which is an advantage for exploration and rescue. In case of loss of robots, scalability would maintain the protection performance to an acceptable level.
	
	\section{The Hardware}
	
	The following sections will go through the details of the robots we used to conduct our experiments, and the prototype enabling swarm control.
	
		\subsection{E-puck}
		\label{sec:e-puck}
		
		The robotic platform chosen was the e-puck \citep{mondada2009puck}. This robot model was made for educational purposes, at university level. Its shape is cylindrical with a diameter of 7.5 cm. It is moved by two diametrically opposed wheels. The figure shows an e-puck from the laboratory. Several extensions were plugged onto it to increase its capabilities. The important sensors that were utilised are the proximity sensors, the omnidirectional camera sensor and the virtual ground sensor. The proximity sensors are based on 8 infrared emitting pieces that return a value proportional to the proximity of an nearby obstacle in the $[O;1]$ interval. These pieces are almost regularly placed a along the perimeter of the robot. The omnidirectional camera is a vertical camera on top of the apparatus, aiming at a convex mirror to provide a 360° view of the environment. It translates this view into a list of colour blobs. A colour blob is a cluster of pixels presenting close colour characteristics, that is, almost being of the same colour (the degree of similarity can be tuned during calibration). During the calibration, among other parameters, one can tune the degree of similarity, the minimum size of the cluster, and the recognised colours. The last sensor is a bit special: it is not real, and not physically present on the board. It is simulated through the ARGoS simulator used to develop the controller of the robot. It sends data created inside the simulator to the robot, from the simulated environment. In our case, this \enquote{simulated/false data} contains the colour of the ground, symbolising the presence of danger. We actually simulated red discs on the floor through the simulator to artificially set up dangerous areas that were not visible by the testing user. That way, the conditions of real life application were the closest possible to ours. An example of red zone is in figure \ref{fig:e-puck_red_zones}.
		
		\begin{figure}[!h]
			\begin{minipage}[c]{0.49\textwidth}
				\includegraphics[height=7cm]{images/e-puck.jpg}			
			\end{minipage}
			\begin{minipage}[c]{0.49\textwidth}
				\includegraphics[width=\textwidth]{../Experiments/No_Human/red_circle.png}			
			\end{minipage}
			
			\caption{The E-puck and its Virtual Sensor}{On the left, one can see a picture of the robotic platform we used: the E-puck. Our swarm is composed of several robot like this one. On the right is a screen capture of the ARGoS simulator \citep{pinciroli2012argos} where the danger virtual sensor is used. Virtual means that it is not real, not physically present on the board. It is simulated through the ARGoS simulator. It sends data created inside the simulator to the robot, from the simulated environment. It augments the robot sensing capabilities. In our case, this \enquote{simulated/false data} contains the colour of the ground, symbolising the presence of danger. The red circle represents a danger zone in which no human can go. No human can see it though. The small dots are representations of the real robots inside the simulator.}
			\label{fig:e-puck_red_zones}
		\end{figure}
		
		\subsection{E-geta} % Longer text than for e-puck because more important!
		\begin{comment}
		[I will in this section describe the need for a device to detect a human and its development. What are the objectives of the hardware? The choices we made to get the final solution. How we built it. Calibration an adaptation process to make the current algorithm compatible with the new hardware. Where does the term come from?]
		\end{comment}
		
	As explained in chapter \ref{chap:escorting_swarm}, one of the main issue was to enhance the robots so they could detect the user and position themselves with respect to him without any large external equipment. Large external equipments are not recommended since, in the targeted unknown environment, they might not be usable. For example, one might use a tracking system to get the position of the robots in real time and communicate it to the robots for them to adjust their speed. In controlled environment, this may work very well, but in the field it would be difficult to access that knowledge.\\
	
	We thus opted for a compact, wearable device that would act as a \enquote{landmark} for the robots: a pair of shoes. In order for the robots to understand on which side of the human they were (to go in front of the user), the two shoes had to emit a different message. The first idea the team had was to make use of the range and bearing sensors of the e-pucks to interact with the shoes. The shoes would have been equipped with infrared emitters. Unfortunately, tests with real robots demonstrated the inability of the range and bearing sensors to evaluate their distance to the human with precision. This method was abandoned for another one, more reliable: LEDs.\\
	
	The omnidirectional camera sensor, explained in section \ref{sec:e-puck}, provided more accurate data. After validation of this method by tests on the real robots, the decision was made to create a prototype of a pair of shoes equipped with colour lights. Both shoes had to emit a different colour to give the robot information on the direction of the human. In section \ref{sec:robot_behaviour} we come back on the algorithm used to deduce the direction of the human from the observed colours.\\
	
	\begin{comment}
	[Inspiration des getas. Plan ou modèle 3D. Méthode de construction (laser cut pour les plexi etc). Circuit électronique.]
	\end{comment}
	
	\begin{figure}
		\begin{minipage}[c]{0.49\textwidth}
			\includegraphics[width=\textwidth]{images/512px-Geta.jpg}
		\end{minipage}
		\hfill
		\begin{minipage}[c]{0.49\textwidth}
			\includegraphics[width=\textwidth]{images/shoes2.jpg}
		\end{minipage}
		
		\caption{E-Geta}{Left image by Haragayato [GFDL (http://www.gnu.org/copyleft/fdl.html) or CC-BY-SA-3.0 (http://creativecommons.org/licenses/by-sa/3.0/)], via Wikimedia Commons. Found on \citet{wiki:001}. On the right is a picture of our prototype shoes, connected to their battery through voltage regulators. See figure \ref{fig:circuit} for the circuit.}
		\label{fig:e-geta}
	\end{figure}
	
	The laboratory took inspiration from the Japanese \enquote{geta} shoes for the design. Figure \ref{fig:e-geta} shows a picture of a Japanese wooden shoe called geta. The right side of the figure is a picture of our prototype. Both have the same overall design. We chose this design in order to slow down the human's speed. Indeed, he speed of the robots is limited to maximum 10 cm/s for per wheel. Another advantage is the simplicity of the structure, and the low number of assembly parts. Figure \ref{fig:e-geta_blueprints} details the blueprints of the prototype.\todo{Insert blueprints}
	
	\begin{figure}
		Insert blueprints
		
		\caption{E-Geta Blueprints}{Insert text}
		\label{fig:e-geta_blueprints}
	\end{figure}
	
	The base of the shoe is made with wood. The surrounding piece covering the LEDs was cut in sheets of semi-opaque plexiglass to diffuse the light. The LEDs are standard strip LEDs (one red strip, and one green). The final electronic circuit is very simple. The 9 volts battery is connected to two variable voltage regulators (step up) in parallel to increase the potential on the output. Each regulator is connected to a shoe LED strip. A separation is necessary since the green LED strip requires more energy than the red one. To get an equivalent luminosity for both shoes, a different voltage had to be applied.
	
	\begin{figure}\centering
		\includegraphics[scale=0.5]{figures/circuit.png}
		\caption{The Circuit}{The figure shows a sketch of the final electronic circuit for the shoes. The battery delivers 9 volts to two regulators in parallel, one for each shoe since each one of them need a different energy supply. Indeed, the green LED strip is more power hungry than the red one. The mass (black cable) is common for all the circuit.}
		\label{fig:circuit}
	\end{figure}
	
	\section{The Robot Behaviour}
	\label{sec:robot_behaviour}
	
	\begin{comment}
		Pour parvenir à mes fins, j'ai dû implémenter un controller. J'ai choisi virtual physics car ces avantages et inconvénients. C'est ce gars qui l'a inventé, et voici les travaux utilisant cette technique.
		0) Aspect solution avec cercle.
		1) Dois faire un controller en c++, pour la plateforme ARGoS, mais aussi compatible e-puck.
		2) Design de la state machine.
		3) Virtual physics. Les forces pour chaque state, l'idée de l'algorithme derrière, et leur évolution.
		4) Les paramètres les plus importants ? Le fait qu'on peut les changer.
		5) Comment traduire le vecteur de direction en vitesse de roue.
	\end{comment}
	
	The first step of the design of the solution is to imagine how the system will look like and how we will implement it, the overall behaviour of the swarm. How do the robots move around the human? What shape will they try to form? This part is important because it will define the overall look and performance of the system.\\
	
	The first shape that intuitively comes in mind is the circle. The circle is the most elementary shape in geometry. It offers the best ratio:
	$$\frac{\mbox{\textit{Surface}}}{\mbox{\textit{Perimeter}}} = \frac{\pi r^2}{2\pi r} = \frac{r}{2}~\mbox{, where r is the radius of the circle.}$$
	
	That means that fewer robots are needed for the same protected area, and more space for the human with a certain amount of robots than any other possible shape. Luckily, it is also the easiest shape to realise in practice. The figure \ref{fig:circle_shape} represents the kind of circle that we would like to obtain for 6 robots and 1 human in the centre. All the robots are equally distanced from each other and the human. The human is protected in all directions. In presence of danger, the robots in contact with it should report it to the human and prevent him from going towards it, as seen on figure \ref{fig:swarm_preventing}. In that case, the conditions concerning the target distance from the human and between robot may not be respected.\\
	
	\begin{figure}[!h]\centering
	\begin{tikzpicture}
		\draw [dashed] (0,0) circle [radius=4];
		\epuck{4}{0}
		\epuck{2}{3.464}
		\epuck{-2}{3.464}
		\epuck{-4}{0}
		\epuck{-2}{-3.464}
		\epuck{2}{-3.464}
		
		\human{0}{0}
	\end{tikzpicture}
	\caption{Ideal Behaviour in Absence of Danger}{This figure symbolises the ideal behaviour required in absence of danger. The swarm forms a circle to cover the widest protected surface for a given amount of robots. All the robots are equally distanced from each other and the human. The human is protected in all directions. In presence of danger, the robots in contact with it should report it to the human and prevent him from going towards it, as seen on figure \ref{fig:swarm_preventing}. In that case, the conditions concerning the target distance from the human and between robot may not be respected.}
	\label{fig:circle_shape}
	\end{figure}	
	
	Implementing a robots swarm behaviour means writing a controller code for its individual components: the robots. The laboratory provides a template in the Lua and the C++ languages for this purpose. The logic of the individual behaviour is added inside callback methods called either by the simulator when performing simulations inside ARGoS \citep{pinciroli2012argos}, or by the robot main method when testing on real robots. The first attempts to build the core of the controller were made in Lua to accelerate the process of trial and error. Indeed, Lua has a simpler syntax, is interpreted by the simulator and does not require any compilation process. Furthermore, the debugging is faster since the simulator includes an editor for the lua controller code. Although Lua is very convenient for quick implementation, it is not supported by the robots. Hence a translation was necessary to port the code to C++ in order to begin the tests on the real robots. After that, the implementation continued exclusively in C++ since the tests on the robots were more frequent. The C++ template developed by the laboratory can be compiled for the ARGoS simulator and cross-compiled for the real robots (e-puck) without any modification. After the compilation, a simple transfer of the binary codes over WiFi allows the operator to store the controller on the robots to launch an experiment.\\
	
	The final implementation of the controller is built on 2 layers. The upper layer is a deterministic finite state machine, containing for each state a specific behaviour in the lower layer. Figure \ref{fig:state_machine} illustrates the whole structure of the upper layer. Figure \ref{fig:state_machine} (left) is what could be called the \enquote{core} of the behaviour, while fig. \ref{fig:state_machine} (right) would be considered as additions to enhance the behaviour. The core part of the state machine is present on fig. \ref{fig:state_machine} (right) since it is one of its constituents. One can only limit himself to core part to understand the behaviour. It is composed of 3 states: \emph{Searching} when the robots do not detect any human nor obstacle, \emph{Escorting} when a human is detected, and \emph{Protection} when a human is detected and there is a danger nearby. If an obstacle is detected by a robot, the latter escapes from the core behaviour of the state machine and enters the \emph{O.A. (Obstacle Avoidance} state. If the robot is blocked for too long, another state takes over to unblock it: \emph{Unblocking}. When no more obstacle is in front of the robot, it can go back to its obstacle avoidance if any obstacle remains close. If none, it switches back to its core actions. The labels next to each transition must be read as follows: \emph{H(uman)}, \emph{D(anger)}, \emph{O(bstacle)}, \emph{F(ront obstacle)}, \emph{T(hreshold crossed, stuck for too long and initiating rotation procedure to escape from the obstacles)}. \emph{N} stands for the negation, so \emph{NH} means \enquote{no human found}. A complete state machine gathering all the states can be found in appendix \ref{app:complete_state_machine}. Although the complete state machine is containing all the aspects of the behaviour, it does not allow to grasp the idea quickly. Dividing the final behaviour into several connected sub-behaviours modularises the solution. Adding a new state, a new sub-behaviour is easy.\\
	
	\begin{figure}[!h]\centering
		\begin{minipage}[c]{.49\textwidth}
			\begin{tikzpicture}[->, >=stealth', shorten >=1pt, auto, node distance=4cm, semithick]
				\node[initial,state] (RW)                    {Searching};
		  		\node[state]         (E) [below right of=RW] {Escorting};
		  		\node[state]         (P) [below left of=RW] {Protection};
		  		
		  		\path	(RW)	 edge[bend left] node{\color{gray} H} (E)
		  				(RW)	 edge node{\color{gray} H\&D} (P)
		  				(E) edge node {\color{gray} NH}	(RW)
		  				(E) edge[bend left] node {\color{gray} D}	(P)
		  				(P) edge node {\color{gray} ND}	(E)
		  				(P) edge[bend left] node[yshift=-0.2cm] {\color{gray} NH}	(RW);

			\end{tikzpicture}
			
%			\resizebox{\textwidth}{!}{%
		  				
		\end{minipage}
		\hfill
		\begin{minipage}[c]{.49\textwidth}
		\resizebox{\textwidth}{!}{%
			\begin{tikzpicture}[->, >=stealth', shorten >=1pt, auto, node distance=4cm, semithick]
				\node[initial,state] (C)                    {Core};
		  		\node[state]         (O) [above right of=C] {O.A.};
		  		\node[state]         (U) [above left of=C] {Unblocking};
		  		
		  		\path	(C)	edge[bend right] node[below,xshift=0.1cm]{\color{gray} O} (O)
		  				(O) edge node[above,xshift=-0.2cm] {\color{gray} NO}	(C)
		  				(O) edge[bend right] node {\color{gray} T}	(U)
		  				(U) edge node[below] {\color{gray} NF\&O}	(O)
		  				(U) edge[bend right] node[yshift=-0.2cm] {\color{gray} NF}	(C);

			\end{tikzpicture}}
		\end{minipage}
		
		\caption{State Machine of the Final Behaviour}{This figure is the visual representation of the state machine of the final behaviour. Figure \ref{fig:state_machine} (left) is what could be called the \enquote{core} of the behaviour, while fig. \ref{fig:state_machine} (right) would be considered as additions to enhance the behaviour. The core part of the state machine is present on fig. \ref{fig:state_machine} (right) since it is one of its constituents. The abbreviation O.A. means \enquote{obstacle avoidance}. The labels next to each transition must be read as follows: H(uman), D(anger), O(bstacle), F(ront obstacle), T(hreshold crossed, stuck for too long and initiating rotation procedure to escape from the obstacles). N stands for the negation, so NH means \enquote{no human found}.}
		\label{fig:state_machine}
	\end{figure}

	Once the actions the robots have to execute have been defined, we have to implement them, code them in the controller that will be run. So the next step was to find a way to translate those actions into code. Our behaviour is a kind of coordinated motion and pattern formation. Thus the intuitive way of implementing it was to make use of the virtual physics design. Using this framework, each robot is a particle subject to virtual forces exerted by the environment (the other robots, the obstacles, and other elements). \citet{khatib1986real} was among the first to use this method. His goal was to implement an obstacle avoidance swarm behaviour where the obstacles create repulsive forces and the goals attractive forces. The overall resulting potential presented local minima at goals and maxima at obstacles. \citet{reif1999social} introduces \enquote{social potential fields} consisting in virtual forces applied on robots by other robots, obstacles, objectives,... The robot resultant motion is defined by the sum of all the forces applied to it. The individual robots carry out the calculations themselves, so the final control is completely distributed. The laws they used are similar to those found in molecular dynamics (inverse-power laws). For example, a law could favour attraction over long distances and repulsion over short distances. One of these is the Lennard-Jones potential, depicted in figure \ref{fig:lennard-jones_potential}. Inverse-power laws, while being extremely simple, can form interesting and elaborate patterns with molecules and plasma gases \citep{reif1999social}. \citet{spears2004distributed} proposed a framework they call \enquote{physicomimetics} to grant distributed control over a large swarm of robots with \enquote{artificial physics}.\todo{More works on virtual physics?}\\
	
	The laws of physics force a system to go to a state of minimum energy, i.e., to reach a minimum of the potential function of the system. Since the force exerted on the system is proportional to the derivative of the corresponding potential -- $$\vec{f} = -\vec{\nabla}P\mbox{,}$$ with $\vec{\nabla}$ being the nabla operator for the gradient computation, $P$ the potential and $\vec{f}$ the force -- the minimum of the potential function means the disappearance of the forces. For every robot to be at the desired location, the forces need to disappear. In this thesis, we only consider forces and not the virtual potentials associated to them. The implemented behaviour is expressed by in terms of virtual forces.\\
	
	\begin{figure}
	\begin{minipage}[c]{0.6\textwidth}
		\begin{tikzpicture}
			\begin{axis}[
			    samples=100,
			    domain=0:1, xmax=1,
			    restrict y to domain=-4:10,
			    axis lines=left,
			    y=1cm/2,
			    x=10cm,
			    grid=both,
			    xtick={0,0.1,...,1},
			    ytick={-4,-2,...,10},
			    compat=newest,
			    xlabel=$d$, xlabel style={at={(1,0)}, anchor=west},
			    ylabel=$f$, ylabel style={rotate=-90,at={(0,1)}, anchor=south}
			]
			\addplot [red, thick] {2.5*((0.3/x)^12 - 2*(0.3/x)^6)};
			\end{axis}
		\end{tikzpicture}
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.4\textwidth}
		$$f(d) = \epsilon \left[ \left(\frac{\sigma}{d}\right)^{12} - 2 \left(\frac{\sigma}{d}\right)^6 \right]$$
		$$\epsilon = 2.5$$
		$$\sigma = 0.3$$
	\end{minipage}
		
		\caption{The Lennard-Jones Potential}{This figure shows a graph of one of the most used virtual potentials in virtual physics, the Lennard-Jones potential, where $\epsilon$ is the gain, $\sigma$ is the target distance and $d$ is the current real distance. The equilibrium is reached at the global minimum at $0.3$.}
		\label{fig:lennard-jones_potential}
	\end{figure}
	
	Using virtual physics offers some advantages over the other methods \citep{brambilla2013swarm}:
	
	\begin{enumerate}
		\item Only one mathematical formula fluently and elegantly converts all the inputs into outputs for the actuators. It removes the need for multiple rules and behaviours.
		$$f:\mathbb{R}^m \rightarrow \mathbb{R}^n:x_1,x_2,...,x_m \rightarrow y_1,y_2,...,y_n = f(x_1,x_2,...,x_m)\mbox{,}$$ where $m$ is the number of inputs, $n$ is the number of outputs and $f$ is the translating function.
		
		\item Multiple behaviours can be combined by simply summing the corresponding resulting vectors.
		$$y_1,y_2,...,y_n = g(x_1,x_2,...,x_m) = \sum_{i=0}^{s}{f_i(x_1,x_2,...,x_m)}\mbox{,}$$ where $s$ is the number of behaviour components.
	\end{enumerate}
	
	One disadvantage is that it might be difficult to find an expression that implements perfectly the behaviour we want.\\
	
	As written above, the robots need inputs to compute the values to send to the actuators, i.e., the wheels. The two types of inputs are the \emph{colour blobs} and the \emph{proximity sensor values}, respectively provided by the omnidirectional camera and the proximity sensors. Both are explained in section \ref{sec:e-puck}. 3 blob colours are used for our solution: red, green and blue, the three basic components of every colour in computer graphics. It was decided to a low number of colours to ease the calibration process and lower the amount of errors when detecting the blobs (wrong colour). To each blob is associated a distance - angle couple. The angle is taken from the front of the robot in radians. With these two value, the robot is able to situate all the blobs and use them as attractive or repulsive points. The proximity values are a list of 8 angle - value couples, where the angle is the position of the sensor on the perimeter of the robot. The whole perimeter of the robot is covered to detect any nearby obstacle. The value is comprised between 0 and 1, inversely proportional to the distance to the obstacle.\\
	
	The following paragraphs explain in details the various forces that were implemented to obtain the desired behaviour from the fed inputs. They are grouped by states of the state machine in which they are used (see fig. \ref{fig:state_machine}). As explained earlier, each state in the upper layer of the general behaviour (the state machine) contains a sub-behaviour (a particular action) executed by means of virtual physics.
		
			\paragraph{Searching}
			
				When no human nor obstacle is around, the robot enters in the \emph{Searching} mode with its LED off. It tries to stay at a constant distance from a detected colour blob, whatever colour it is. Since robots in \emph{Escorting} or \emph{Protecting} mode light their LEDs in blue, the searching robots always stay around the robots helping the human, whom they will detect at some point. This measure prevents the robots from going away too far from the human. This sub-behaviour is implemented through a sum of simplified Lennard-Jones virtual forces, one for each colour blob detected. The term \enquote{simplified} is used because the force is not the real derivative of the Lennard-Jones potential, but a simplified version with lower exponents on the fractions\todo{Why a simplified version? Put the development.}:
				\begin{equation}
					\vec{f}(d) = \frac{-4\epsilon}{d} \left[ \left(\frac{\sigma}{d}\right)^{4} - \left(\frac{\sigma}{d}\right)^2 \right] ~\vec{1_s}
					\label{eq:lennard-jones_force_simplified}
				\end{equation}
				
				The original version is:
				\begin{equation}
					\vec{f}(d) = \frac{-12\epsilon}{d} \left[ \left(\frac{\sigma}{d}\right)^{12} - \left(\frac{\sigma}{d}\right)^6 \right] ~\vec{1_s}
				\end{equation}
				 The simplified version is exposed in figure \ref{fig:lennard-jones_force_simplified}. If no blob is detected, the robot goes forward with a speed of 5 cm/s. The unit vector $\vec{1_s}$ is heading towards the source of the force (the applier).
				
				\begin{figure}\centering
					\begin{tikzpicture}
						\begin{axis}[
						    samples=50,
						    domain=0:150, xmax=142, ymax=2.5,
						    restrict y to domain=-10:30,
						    axis lines=left,
						    y=1cm/3,
						    x=1cm/10,
						    grid=both,
						    xtick={0,10,...,150},
						    ytick={-10,-8,...,30},
						    compat=newest,
						    xlabel=$d$ (cm), xlabel style={at={(1,0)}, anchor=west},
						    ylabel=$f$, ylabel style={rotate=-90,at={(0,1)}, anchor=south}
						]
						\addplot [red, thick] {-400/x*((50/x)^4-(50/x)^2)};
						\end{axis}
					\end{tikzpicture}
					
					\caption{The Simplified Lennard-Jones Virtual Force}{This figure exposes a simplified version of the Lennard-Jones force derivated from the corresponding potential. Its expression is given by equation \ref{eq:lennard-jones_force_simplified}. The term \enquote{simplified} is used because the force is not the real derivative of the Lennard-Jones potential, but a simplified version with lower exponents on the fractions. The parameters values are: $\epsilon=100$ and $\sigma=50$, and are conform to those used with robots. One can observe the root at 50 corresponding to the state of minimum energy in the system. Above 50, the force is positive, so the robot is attracted by the source applying the force. Below 50, it is negative, so the robot is repulsed from the source of the force.}
					\label{fig:lennard-jones_force_simplified}
				\end{figure}
			
			\paragraph{Escorting}
			
			If a human is found nearby and no danger is around, the controller enters into the \emph{Escorting} state. The robot then tries to stay at a fixed distance from the human and other robots. If all the robots around the human follow the same pattern formation rules, a circle appears encircling him/her. The complete virtual force for this state is the sum of 3 components: the \emph{human force}, the \emph{robot repulsion force} and the \emph{gravity force}. All three components are fed with the detected colour blobs as inputs to evaluate the distances and angles.
			
			\begin{itemize}
				\item The \emph{human force} uses a stronger version of the Lennard-Jones force to keep the robot at a certain distance from the human (see figure \ref{fig:lennard-jones_force_stronger}). The force is stronger in the attraction part, hence the name. The repulsion part is unchanged because of the more interesting asymptotic behaviour at $d=0$, increasing the norm of the force quicker than the linear attraction replacement (in gray). Its expression is given by equation \ref{eq:lennard-jones_force_stronger}. Only the closest human colour blob is fed to the force computation.
				
					\begin{equation}
						\vec{f}(d) = \left\{ \begin{array}{rcl}
						\frac{-4\epsilon}{d} \left[ \left(\frac{\sigma}{d}\right)^{4} - \left(\frac{\sigma}{d}\right)^2 \right] ~\vec{1_s} & \mbox{for} & d < \sigma \\
						15(d - \sigma ) ~\vec{1_s} & \mbox{for} & d \geq \sigma
						\end{array}\right.
						\label{eq:lennard-jones_force_stronger}
					\end{equation}
				
					\begin{figure}\centering
						\begin{tikzpicture}
							\begin{axis}[
							    samples=40,
							    domain=30:140, xmax=142, xmin=30, ymax=14.2,
							    restrict y to domain=-10:30,
							    axis lines=left,
							    y=1cm/5,
							    x=1cm/10,
							    grid=both,
							    xtick={30,40,...,150},
							    ytick={-10,-5,...,30},
							    compat=newest,
							    xlabel=$d$ (cm), xlabel style={at={(1,0)}, anchor=west},
							    ylabel=$f$, ylabel style={rotate=-90,at={(0,1)}, anchor=south}
							]
							\addplot [gray, thick, domain=35:140] {-2000/x*((35/x)^4-(35/x)^2)};
							\addplot [gray, thick, domain=30:35] {(x - 35)*15};
							\addplot [red, thick, domain=30:35] {-2000/x*((35/x)^4-(35/x)^2)};
							\addplot [red, thick, domain=35:40] {(x - 35)*15};
							\end{axis}
						\end{tikzpicture}
						
						\caption{The Stronger Lennard-Jones Virtual Force}{This figure exposes a stronger version of the Lennard-Jones force derivated from the corresponding potential. Its expression is given by the equation system \ref{eq:lennard-jones_force_stronger}. The functions are plotted on the whole domain but only the red part is used. It allows to compare both values for the same distance. The force is stronger in the attraction part, hence the name. The repulsion part is unchanged because of the more interesting asymptotic behaviour at $d=0$, increasing the norm of the force quicker than the linear attraction replacement (in gray). The parameters values are: $\epsilon=500$ and $\sigma=35$, and are conform to those used with robots. One can observe the root at 35 corresponding to the state of minimum energy in the system. Above 35, the force is positive, so the robot is attracted by the source applying the force. Below 35, it is negative, so the robot is repulsed from the source of the force.}
						\label{fig:lennard-jones_force_stronger}
					\end{figure}
				
				\item The \emph{robot repulsion force} maintains a fixed distance between the robots escorting or protecting a human (those with LEDs lit in blue). The simplified Lennard-Jones force is used, given by equation \ref{eq:lennard-jones_force_simplified} and figure \ref{fig:lennard-jones_force_simplified}.
				\item The \emph{gravity force} is the only force that can be deactivated via the experiment configuration file. The closest human colour blob to the human is taken as reference (the closest shoe). Then depending on colour of the blob (shoe), two different things can happen: the blob is red and the robot turns clockwise around it, or the blob is green and the it turns anti-clockwise. The goal is to go in front of the human like if the floor was \enquote{sloped down} to the front of the human, hence the name. Figure \ref{fig:gravity_force_idea} illustrates the idea of the gravity force and figure \ref{fig:gravity_force} its norm. Two robots are both on the opposite side of the human. The left one sees the red shoe as the closest one and turns clockwise heading for the front. The right robot does the opposite with the same intention. This force is expressed in polar coordinates:
				
				\begin{equation}
					\vec{f}(\alpha) = \left\{ \begin{array}{rcl}
						\epsilon ~\vec{1_\theta} & \mbox{for} & \alpha < 0 \\
						-\epsilon ~\vec{1_\theta} & \mbox{for} & \alpha \geq 0
						\end{array}\right.
				\end{equation}
				
				$\vec{1_\theta}$ is the standard axis in polar coordinates ($\vec{1_r},\vec{1_\theta}$) where the origin is the centre of the human. $\epsilon$ is the norm of the force.
				
				\begin{figure}\centering
					\begin{tikzpicture}						
						\draw[red, fill] (0,3) arc (90:270:3cm);	
						\draw[LimeGreen, fill] (0,3) arc (90:-90:3cm);
						
						\draw[<-, very thick, rotate around={5:(0,0)}] (0,2) arc (90:250:2cm);
						\draw[<-, very thick, rotate around={-5:(0,0)}] (0,2) arc (90:-70:2cm);
						
						\epuckblue[30]{2.6}{1.5}
						\epuckblue[30]{-2.6}{-1.5}
						
						\human{0}{0}
					\end{tikzpicture}
					
					\caption{The Gravity Virtual Force Concept}{This figure illustrates the idea of the gravity virtual force, the only force that can be disabled in the configuration of the experiment. The closest human colour blob to the human is taken as reference (the closest shoe). Then depending on colour of the blob (shoe), two different things can happen: the blob is red and the robot turns clockwise around it, or the blob is green and the it turns anti-clockwise. The goal is to go in front of the human like if the floor was \enquote{sloped down} to the front of the human, hence the name. Two robots are both on the opposite side of the human. The left one sees the red shoe as the closest one and turns clockwise heading for the front. The right robot does the opposite with the same intention.}
					\label{fig:gravity_force_idea}
				\end{figure}
				
				\begin{figure}\centering
					\begin{tikzpicture}
						\begin{axis}[
						    samples=40,
						    domain=-180:180, xmax=180, ymax=10,ymin=-10,
						    restrict y to domain=-10:10,
						    axis lines=left,
						    y=1cm/5,
						    x=1cm/30,
						    grid=both,
						    xtick={-180,-150,...,180},
						    ytick={-10,-5,...,10},
						    compat=newest,
						    xlabel=degrees, xlabel style={at={(1,0)}, anchor=west},
						    ylabel=$f$, ylabel style={rotate=-90,at={(0,1)}, anchor=south}
						]
						\addplot [red, thick, domain=-180:0] {5};
						\addplot [red, thick, domain=0:180] {-5};
						\end{axis}
					\end{tikzpicture}
					
					\caption{The Gravity Virtual Force}{This depicts the adopted value for the gravity virtual force with respect to the angle from the front of the human. Any angle outside this domain can fall back in the $[-180;180]$ interval by normalising it. An angle of 0° means that the robot is in front of the human. The angle increases by turning anti-clockwise.}
					\label{fig:gravity_force}
				\end{figure}
			\end{itemize}
			
		
			\paragraph{Protection}
			
			The \emph{Protection} is reached when the robot detects a human and a danger. It makes its blue LEDs blink and stays at the border of the encountered danger area while maintaining the escorting formation. Hence this sub-behaviour is based on the one from the \emph{Escorting} state. The only difference is in the \emph{human virtual force}. Since everything stays the same as in \emph{Escorting}, except the fact that the robot must not cross the danger border, we just modify the target distance from the human. In presence of a danger, $\sigma$ (the target distance) will be decreased incrementally until no danger is detected any more. As a result, the robot gets closer to the human like seen on figure \ref{fig:swarm_preventing} at page \pageref{fig:swarm_preventing}. We speak about \emph{dynamic target distance}. Figure \ref{fig:dynamic_target_distance} illustrates the concept. On the figure, the same robot is represented at two different time steps. On the right, the robot is inside the dangerous area after the human took a step towards it. The robot enters \emph{protection mode} and starts blinking. Its human target distance begins to decrease incrementally at each time step by a user-defined amount. As a consequence, it comes closer to the human. When it crosses the border of the danger zone, it goes back to \emph{Escorting} mode (the danger is not detected any more) and raises its target distance again. At some point, it will re-enter in the area and the whole process will start over. To avoid human confusion regarding the presence of danger, there is a delay before the robot stops blinking. That way, when the robot oscillates around the border, it keeps blinking to report the close danger.
			
			\begin{figure}\centering
				\begin{tikzpicture}
					
					\draw[white, fill] (0,-1) rectangle (4,3);
					\draw[red, fill] (4,-1) rectangle (8,3);
					
					\draw[->, very thick] (1,2.5) -- (3.5,2.5);
					\draw[<-, very thick] (4.5,2.5) -- (7,2.5);
					
					\draw (2,-0.5) node[scale=1]{Target $\nearrow$};
					\draw (6,-0.5) node[scale=1]{Target $\searrow$};
					
					\draw[dashed, LightGray] (4,3) -- (0,3) -- (0,-1) -- (4,-1);
				
								\draw[dashed, very thick] (4,-1.5) -- (4,3.5);
					
					\draw (4.5,3.5) node[scale=1, anchor=west]{\textsc{Danger}};
					\draw (3.5,3.5) node[scale=1, anchor=east]{\textsc{Safe}};
					
					
					\human[-90]{-1}{1}
					\epuckblue[-90]{2}{1}
					\epuckblue[90]{6}{1}
					
					\draw (4,1) node[scale=2, anchor=east]{$\leadsto$};
					\draw (4,1) node[scale=2, anchor=east, rotate=180]{$\leadsto$};
					
					\foreach \i in {15,45,...,345} {
				        \draw [very thick, white, rotate around={\i:(6,1)}] (6.6,1) -- (6.9,1);
				    }
								
				\end{tikzpicture}
				
				\caption{The Dynamic Target Distance}{The concept of dynamic target distance is explained in this drawing. On the figure, the same robot is represented at two different time steps. On the right, the robot is inside the red dangerous area after the human took a step towards it. The robot enters \emph{protection mode} and starts blinking. Its human target distance begins to decrease incrementally at each time step by a user-defined amount. As a consequence, it comes closer to the human. When it crosses the border of the danger zone, it goes back to \emph{Escorting} mode (the danger is not detected any more) and raises its target distance again. At some point, it will re-enter in the area and the whole process will start over. To avoid human confusion regarding the presence of danger, there is a delay before the robot stops blinking. That way, when the robot oscillates around the border, it keeps blinking to report the close danger.}
				\label{fig:dynamic_target_distance}
			\end{figure}
			
			\paragraph{Obstacle Avoidance}
			
			The \emph{Obstacle Avoidance} is activated when the robot encounters an object. Since the robot cannot go back, only the front sensors are used. Figure \ref{fig:obstacle_avoidance} shows the four sensors that are taken into account. Each sensor is seen as a force pushing the robot whose intensity is inversely proportional to the distance to the sensed object. If the norm of the strongest force is over a threshold, the robot goes into \emph{Obstacle Avoidance} and uses the resultant as direction vector.
			
			\begin{figure}\centering
				\begin{tikzpicture}
					
					\draw [very thick, fill=white] (0,0) circle [radius=1.5];
					\draw [very thick] (-0.75,-3*0.433) -- (0,1.45) -- (0.75,-3*0.433);
					
					\draw [<-,very thick,red,rotate around={157.5:(0,0)}] (1.5,0) -- (2.5,0) node [left] {$\vec{f_1}$};
					\draw [<-,very thick,red,rotate around={112.5:(0,0)}] (1.5,0) -- (2.5,0) node [above] {$\vec{f_2}$};
					\draw [<-,very thick,red,rotate around={67.5:(0,0)}] (1.5,0) -- (2.5,0) node [above] {$\vec{f_3}$};
					\draw [<-,very thick,red,rotate around={22.5:(0,0)}] (1.5,0) -- (2.5,0) node [right] {$\vec{f_4}$};
				
				\end{tikzpicture}
				
				\caption{The Obstacle Avoidance Concept}{This figure shows the four sensors that are taken into account to detect a nearby object. Each sensor is seen as a force pushing the robot whose intensity is inversely proportional to the distance to the sensed object. If the norm of the strongest force is over a threshold, the robot goes into \emph{Obstacle Avoidance} and uses the resultant as direction vector.}
				\label{fig:obstacle_avoidance}
				\end{figure}
			
			\paragraph{Unblocking}
			
			If the robot stays in \emph{Obstacle Avoidance} and changes its direction for some time, it enters into \emph{Unblocking} mode. Figure \ref{fig:unblocking} shows a situation likely to provoke it. The robot enters a narrow path between obstacles. Let us say that it is closer to the left object and goes towards it. Since the forces generated by the proximity sensors on the left are stronger, the resultant is heading to the right, bringing the robot to the other obstacle. The same event occurs on the right side, and the robot returns to the left obstacle. If the robot keeps doing for a certain amount of time, the \emph{Unblocking} mode is activated. At that point, two different things can occur: an obstacle lies ahead of the robot within a certain distance, or nothing is in front of the robot.
			\begin{itemize}
				\item If there is nothing, the robot moves forward for one time step, and returns to the appropriate state: \emph{Obstacle Avoidance} if there is an obstacle around, or a state of the \emph{core} if none.
				\item If there is an obstacle in front, the robot turns on itself until there is none. It is the case in figure \ref{fig:unblocking}.
			\end{itemize}
			
			
			\begin{figure}\centering
				\begin{tikzpicture}					
					
					\draw [Sienna, fill] (-4,2) -- (-4.3,1.7) -- (-4.3,-2.3) -- (-4,-2) -- cycle;
					\draw [Sienna, fill] (-4,-2) -- (-4.3,-2.3) -- (-2.3,-2.3) -- (-2,-2) -- cycle;
					
					\draw [Sienna, fill] (2,2) -- (1.7,1.7) -- (1.7,-2.3) -- (2,-2) -- cycle;
					\draw [Sienna, fill] (2,-2) -- (1.7,-2.3) -- (3.7,-2.3) -- (4,-2) -- cycle;
					
					\draw [Sienna, fill] (-1,4) -- (-1.3,3.7) -- (-1.3,1.7) -- (-1,2) -- cycle;
					\draw [Sienna, fill] (-1,2) -- (-1.3,1.7) -- (0.7,1.7) -- (1,2) -- cycle;
					
					\draw [<-,very thick,red,rotate around={112.5:(0,0)}] (1.5,0) -- (2.1,0);
					\draw [<-,very thick,red,rotate around={67.5:(0,0)}] (1.5,0) -- (2.1,0);
					
						
					\draw [SeaGreen, fill] (-0.3,-0.3) circle [radius=1.5];
					\draw [SeaGreen, fill] (-1.0606601715,1.0606601715) -- (-1.0606601715-0.3,1.0606601715-0.3) -- (-0.3+1.0606601715,-0.3-1.0606601715) -- (1.0606601715,-1.0606601715) -- cycle;
					\draw [very thick, fill=white] (0,0) circle [radius=1.5];
					\draw [very thick] (-0.75,-3*0.433) -- (0,1.45) -- (0.75,-3*0.433);
					\draw[very thick, fill=white] (-1.7,-0.3) ellipse (0.1cm and 0.5cm);
					\draw[very thick, fill] (-1.7,-0.3) ellipse (0.01cm and 0.05cm);
					
					\draw[brown, fill] (-2,-2) rectangle (-4,2);
					\draw[brown, fill] (2,-2) rectangle (4,2);
					\draw[brown, fill] (-1,2) rectangle (1,4);
					
					\draw[->,line width=0.1cm, purple, >=latex, rotate around={-15:(0,0)}] (0,1.2) arc (90:-240:1.2);
					
				\end{tikzpicture}
				
				\caption{The Unblocking Concept}{On this figure, the robot is trapped between two obstacles, one on the left and the other on the side. Let us say that it is closer to the left object and goes towards it. Since the forces generated by the proximity sensors on the left are stronger, the resultant is heading to the right, bringing the robot to the other obstacle. The same event occurs on the right side, and the robot returns to the left obstacle. If the robot keeps doing for a certain amount of time, the \emph{Unblocking} mode is activated. At that point, two different things can occur: an obstacle lies ahead of the robot within a certain distance, or nothing is in front of the robot. If there is nothing, the robot moves forward for one time step, and returns to the appropriate state: \emph{Obstacle Avoidance} if there is an obstacle around, or a state of the \emph{core} if none. If there is an obstacle in front, the robot turns on itself until there is none. Here, it will turn until heading south of the image, and then get out.}
				\label{fig:unblocking}
			\end{figure}
		
	All the parameters presented until now can be tuned by the human to ensure the best performance. Here is a list of the important parameters available inside the configuration file:
		
	\begin{description}
		\item[Human Force Gain:] The factor $\epsilon$ multiplying the force in expression \ref{eq:lennard-jones_force_stronger}. Increasing it strengthens the force the human exerts on the robots.
		\item[Human Force Distance:] The target distance to the human for all the robot escorting him/her.
		\item[Human Force Distance Variation Delta:] The value added or subtracted to the target distance to run the dynamic target distance mechanism.
		\item[Agent Force Gain:] The factor $\epsilon$ multiplying the force in expression \ref{eq:lennard-jones_force_simplified}. Increasing it strengthens the force robots exert on each other.
		\item[Agent Force Distance:] The target distance all the robots must keep between each other.
		\item[Gravity Force:] A boolean activating the \emph{gravity force} that pushes robots in front of the human.
		\item[Gravity Force Gain:] The norm of the force that pushes the robots in front of the human.
		\item[Direction Vector Window Size:] The direction vector sent to the wheels is computed by averaging a number of direction vectors: the one returned from the forces exerted on the robot at this time step, and a number of previous vectors sent to the wheels. The parameter gives the amount of vectors used for the average. Increasing it makes the motion smoother.
	\end{description}
	
	Once the direction vector has been computed by the robot, it still has to translate it into wheel speeds. The robotic platform we used is the E-puck. As mentioned in section \ref{sec:e-puck}, it only has two diametrically opposed wheels. Figure \ref{fig:direction_vector_wheel_speeds_translation} exposes in pseudo code the algorithm behind the conversion.
	
		\begin{algorithm}
			\textcolor{gray}{\tcc{Get the direction values:}}
			\emph{angle} $:=$ angle of the direction vector with respect to the front of the robot\;
			\emph{speed} $:=$ norm of the direction vector\;
			
			\BlankLine
	
			\textcolor{gray}{\tcc{Check if speed is not too high for the robot:}}
			\If{speed > 10}{
				\emph{speed} = 10\;
			}
			
			\BlankLine
			
			\textcolor{gray}{\tcc{Multiply angle by 3 to accelerate the robot rotations:}}
			\emph{angle} = $3~\cdot$ \emph{angle}\;
	
			\BlankLine
			
			\textcolor{gray}{\tcc{Check if angle is still correct:}}
			\If{angle > $\pi$}{
				\emph{angle} = $\pi$\;
			}
			\If{angle < $-\pi$}{
				\emph{angle} = $-\pi$\;
			}
			
			\BlankLine
			
			\textcolor{gray}{\tcc{Assign wheels speed:}}
			\eIf{0 < angle < $\pi$}{
				\emph{leftWheelSpeed} = \emph{speed} $\cdot\cos($\emph{angle}$)$\;
				\emph{rightWheelSpeed} = \emph{speed}\;
			}{
				\emph{leftWheelSpeed} = \emph{speed}\;
				\emph{rightWheelSpeed} = \emph{speed} $\cdot\cos($\emph{angle}$)$\;
			}
			
					\caption{The Direction Vector to Wheel Speeds Translation}{This pseudo code explains how the computed direction vector is translated into the robot wheel speeds.}
		\label{fig:direction_vector_wheel_speeds_translation}
		\end{algorithm}
		

	


\chapter{Experiments}
\label{chap:experiments}
\begin{comment}
[\textcolor{red}{Explain very clearly both types of tests (no human/human).}]
\end{comment}


	\section{Characterisation of the System}
	[The measurements and the tests on the final behaviour.]

		\subsection{Metrics}
		
		[Metrics I will use. Their description. How I will compute them.]
		
			\paragraph{Correct Distance}
			
			[Do the robots respect the correct wanted distance?]
			
			\paragraph{Robot Density}
			
			[Do the robots surround the human correctly?]
			
			\paragraph{Time}
			
			[Do they do that in a relatively low amount of time?]
			
		\subsection{Set-up}
		
		[How I am performing my experiments.]
		
		\subsection{Analysis}
		
		[All the graphs we discussed about. The evolution of the error over time. The analysis of the behaviour on basis of the criteria we defined.]
		
	\section{Demonstration}
	
	[What demonstration was done with the device. Add pictures. Describe perfectly.]

\chapter{Conclusion}

[I've done this, this and this (~1/2 pages). (Intro: "I'll do this, this...) \textcolor{red}{Put sentences of type "so what?". Continuous text.}

	\paragraph{Future Works}
	[The future works that would be interesting from my point of view.]
	
		\subparagraph{Other Robots}
		\subparagraph{Guidance}
			
			\begin{description}
				\item[Zero Visibility Areas or Blind People:]
				\item[Human Motion Synchronisation:]
				\item[Vehicle Guidance:]
			\end{description}
			


\appendix

\chapter{The Complete State Machine}
\label{app:complete_state_machine}

\begin{figure}[!h]\centering
	\begin{minipage}[c]{.49\textwidth}
%			\begin{tikzpicture}[->, >=stealth', shorten >=1pt, auto, node distance=4cm, semithick]
%				\node[initial,state] (NORMAL)                    {Normal};
%		  		\node[state]         (OBS) [below left of=NORMAL] {Obstacle};
%		  		\node[state]         (HU) [below of=NORMAL] {Human};
%		  		\node[state]         (DEF) [below right of=NORMAL] {Default};
%		  		\node[state]         (BLOCK) [above of=NORMAL]       {Blocked};
%		  		
%		  		\path	(BLOCK)		edge		node					{\color{gray} 1}	(NORMAL)
%		  				(NORMAL)		edge		node[yshift=0.2cm]	{\color{gray} 2}	(OBS)
%		  				(NORMAL)		edge		node					{\color{gray} 3}	(HU)
%		  				(NORMAL)		edge		node	[yshift=-0.2cm]	{\color{gray} 4}	(DEF)
%		  				(OBS)		edge		[bend left]	node		{\color{gray} 5}	(BLOCK)
%		  				(OBS)		edge		[bend right]	node		{}				(NORMAL)
%		  				(HU)			edge		[bend right]	node		{}				(NORMAL)
%		  				(DEF)		edge		[bend right]	node		{}				(NORMAL);
%			\end{tikzpicture}
%			
		\resizebox{\textwidth}{!}{%
		\begin{tikzpicture}[->, >=stealth', shorten >=1pt, auto, node distance=4cm, semithick]
	  		\node[state]         (OBS)					{Obstacle Avoidance};
	  		\node[state]         (HU) [below of=OBS]		{Escorting};
	  		\node[state]         (DEF) [below of=HU]		{Searching};
	  		\node[state]         (BLOCK) [right of=HU]	{Unblocking};
	  		\node[state]         (DANGER) [left of=HU]	{Protection};
	  		
	  		\path	(OBS)		edge	[bend left]	node		{\color{gray} 1}	(BLOCK)
	  				(OBS)		edge	[bend left]	node[yshift=-1.2cm, xshift=-0.3cm]		{\color{gray} 2}				(DEF)
	  				(OBS)		edge	[bend left]	node	[left]	{\color{gray} 3}				(HU)
	  				(HU)			edge	[bend left]	node[right]		{\color{gray} 4}				(OBS)
	  				(HU)			edge	[bend left]	node	[left]	{\color{gray} 2}				(DEF)
	  				(DEF)		edge	[bend left]	node[right]		{\color{gray} 3}				(HU)
	  				(DEF)		edge	[bend left]	node	[right, yshift=0.2cm]	{\color{gray} 4}				(OBS)
	  				(BLOCK)		edge				node		{\color{gray} 5}				(OBS)
	  				(BLOCK)		edge				node	[xshift=0.3cm]	{\color{gray} 6}				(HU)
	  				(BLOCK)		edge	[bend left]			node		{\color{gray} 7}				(DEF)
	  				(BLOCK)		edge	[bend left, above]	node[yshift=-0.1cm]		{\color{gray} 8}				(DANGER)
	  				(HU)		edge	[bend left, above]	node		{\color{gray} 9}				(DANGER)
	  				(DANGER)	 edge[bend left, above] node{\color{gray} 3} (HU)
	  				(DANGER)	 edge[bend left, above] node{\color{gray} 4} (OBS)
	  				(OBS)	 edge[above] node{\color{gray} 9} (DANGER)
	  				(DANGER)	 edge[below] node{\color{gray} 2} (DEF)
	  				(DEF)	 edge[bend left, below] node{\color{gray} 9} (DANGER);
	  				
		\end{tikzpicture}
		}
	\end{minipage}
	\hfill
	\begin{minipage}[c]{.49\textwidth}
		\small\sffamily
		
		\begin{enumerate}
			\item Amount of direction change (left/right) while having an obstacle around reaches a threshold.
			\item No human \& no obstacle.
			\item No obstacle \& human \& no danger.
			\item Obstacle.
			\item No front obstacle \& obstacle elsewhere.
			\item No front obstacle \& human \& no danger.
			\item No front obstacle \& no human \& no obstacle.
			\item No front obstacle \& human \& danger.
			\item No obstacle \& human \& danger.

			\end{enumerate}

%			\begin{enumerate}
%				\item No more obstacle in front.
%				\item Obstacle found around the robot.
%				\item Human found around the robot.
%				\item Neither of the previous criteria met.
%				\item Amount of direction change (left/right) while having an obstacle around reaches a threshold.
%			\end{enumerate}
	\end{minipage}
	
	\caption{State Machine of the Final Behaviour}{This figure is the visual representation of the state machine of the final behaviour. On the left, the states and their connections are drawn. On the right, the information on the conditions needed to take the transitions are listed.}
	\label{fig:state_machine_complete}
\end{figure}

\chapter{E-puck}

\chapter{ARGoS}

\chapter{Arena Tracking System}

\chapter{Range and Bearing}

\chapter{Omnidirectional Camera}

\chapter{Controller Code}

\chapter{MATLAB Scripts Code}

\chapter{Human Detection Devices Blueprints}

\backmatter
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAPHY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{filecontents}{thesis.bib}
%http://en.wikibooks.org/wiki/LaTeX/Bibliography_Management

@inproceedings{podevijn2012self,
  title={Self-organised feedback in human swarm interaction},
  author={Podevijn, Ga{\"e}tan and O’Grady, Rehan and Dorigo, Marco},
  booktitle={Proceedings of the workshop on robot feedback in human-robot interaction: how to make a robot readable for a human interaction partner (Ro-Man 2012)},
  year={2012}
}

@incollection{podevijn2014gesturing,
  title={Gesturing at subswarms: Towards direct human control of robot swarms},
  author={Podevijn, Ga{\"e}tan and O’Grady, Rehan and Nashed, Youssef SG and Dorigo, Marco},
  booktitle={Towards Autonomous Robotic Systems},
  pages={390--403},
  year={2014},
  publisher={Springer}
}

@article{brambilla2013swarm,
  title={Swarm robotics: a review from the swarm engineering perspective},
  author={Brambilla, Manuele and Ferrante, Eliseo and Birattari, Mauro and Dorigo, Marco},
  journal={Swarm Intelligence},
  volume={7},
  number={1},
  pages={1--41},
  year={2013},
  publisher={Springer}
}

@incollection{csahin2005swarm,
  title={Swarm robotics: From sources of inspiration to domains of application},
  author={{\c{S}}ahin, Erol},
  booktitle={Swarm robotics},
  pages={10--20},
  year={2005},
  publisher={Springer}
}

@phdthesis{kazadi2000swarm,
  title={Swarm engineering},
  author={Kazadi, Sanza T},
  year={2000},
  school={California Institute of Technology}
}

@book{minsky1967computation,
  title={Computation: finite and infinite machines},
  author={Minsky, Marvin L},
  year={1967},
  publisher={Prentice-Hall, Inc.}
}

@article{granovetter1978threshold,
  title={Threshold models of collective behavior},
  author={Granovetter, Mark},
  journal={American journal of sociology},
  pages={1420--1443},
  year={1978},
  publisher={JSTOR}
}

@inproceedings{bonabeau1997adaptive,
  title={Adaptive Task Allocation Inspired by a Model of Division of Labor in Social Insects.},
  author={Bonabeau, Eric and Sobkowski, Andrej and Theraulaz, Guy and Deneubourg, Jean-Louis},
  booktitle={BCEC},
  pages={36--45},
  year={1997}
}

@article{bachrach2010composable,
  title={Composable continuous-space programs for robotic swarms},
  author={Bachrach, Jonathan and Beal, Jacob and McLurkin, James},
  journal={Neural Computing and Applications},
  volume={19},
  number={6},
  pages={825--847},
  year={2010},
  publisher={Springer}
}

@article{wolpert1999introduction,
  title={An introduction to collective intelligence},
  author={Wolpert, David H and Tumer, Kagan},
  journal={arXiv preprint cs/9908014},
  year={1999}
}

@article{abbott2006emergence,
  title={Emergence explained: Abstractions: Getting epiphenomena to do real work},
  author={Abbott, Russ},
  journal={Complexity},
  volume={12},
  number={1},
  pages={13--26},
  year={2006},
  publisher={Wiley Online Library}
}

@article{pinciroli2012argos,
  title={{ARGoS}: a Modular, Parallel, Multi-Engine Simulator for Multi-Robot Systems},
  author={Carlo Pinciroli and Vito Trianni and Rehan O'Grady and Giovanni Pini and Arne Brutschy and Manuele Brambilla and Nithin Mathews and Eliseo Ferrante and Gianni {Di Caro} and Frederick Ducatelle and Mauro Birattari and Luca Maria Gambardella and Marco Dorigo},
  journal={Swarm intelligence},
  volume={6},
  number={4},
  pages={271--295},
  year={2012},
  publisher={Springer},
  address = {Berlin, Germany}
}

@inproceedings{daily2003world,
  title={World embedded interfaces for human-robot interaction},
  author={Daily, Mike and Cho, Youngkwan and Martin, Kevin and Payton, Dave},
  booktitle={System Sciences, 2003. Proceedings of the 36th Annual Hawaii International Conference on},
  pages={6--pp},
  year={2003},
  organization={IEEE}
}

@incollection{baizid2009human,
  title={Human multi-robots interaction with high virtual reality abstraction level},
  author={Baizid, Khelifa and Li, Zhao and Mollet, Nicolas and Chellali, Ryad},
  booktitle={Intelligent Robotics and Applications},
  pages={23--32},
  year={2009},
  publisher={Springer}
}

@inproceedings{mclurkin2006speaking,
  title={Speaking Swarmish: Human-Robot Interface Design for Large Swarms of Autonomous Mobile Robots.},
  author={McLurkin, James and Smith, Jennifer and Frankel, James and Sotkowitz, David and Blau, David and Schmidt, Brian},
  booktitle={AAAI Spring Symposium: To Boldly Go Where No Human-Robot Team Has Gone Before},
  pages={72--75},
  year={2006}
}

@inproceedings{mondada2009puck,
  title={The e-puck, a robot designed for education in engineering},
  author={Mondada, Francesco and Bonani, Michael and Raemy, Xavier and Pugh, James and Cianci, Christopher and Klaptocz, Adam and Magnenat, Stephane and Zufferey, Jean-Christophe and Floreano, Dario and Martinoli, Alcherio},
  booktitle={Proceedings of the 9th conference on autonomous robot systems and competitions},
  volume={1},
  number={LIS-CONF-2009-004},
  pages={59--65},
  year={2009},
  organization={IPCB: Instituto Polit{\'e}cnico de Castelo Branco}
}

@misc{wiki:001,
   author = "Wikipedia",
   title = "Geta (footwear) --- Wikipedia{,} The Free Encyclopedia",
   year = "2015",
   url = "\url{http://en.wikipedia.org/w/index.php?title=Geta_(footwear)&oldid=651925222}",
   note = "[Online; accessed 20-May-2015]"
 }
 
 @article{khatib1986real,
  title={Real-time obstacle avoidance for manipulators and mobile robots},
  author={Khatib, Oussama},
  journal={The international journal of robotics research},
  volume={5},
  number={1},
  pages={90--98},
  year={1986},
  publisher={Sage Publications}
}

@article{reif1999social,
  title={Social potential fields: A distributed behavioral control for autonomous robots},
  author={Reif, John H and Wang, Hongyan},
  journal={Robotics and Autonomous Systems},
  volume={27},
  number={3},
  pages={171--194},
  year={1999},
  publisher={Elsevier}
}

@article{spears2004distributed,
  title={Distributed, physics-based control of swarms of vehicles},
  author={Spears, William M and Spears, Diana F and Hamann, Jerry C and Heil, Rodney},
  journal={Autonomous Robots},
  volume={17},
  number={2-3},
  pages={137--162},
  year={2004},
  publisher={Springer}
}

\end{filecontents}

%\nocite{*}
\bibliographystyle{plainnat}
\bibliography{thesis}

\end{document}
